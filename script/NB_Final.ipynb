{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BC5 group S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rever este texto e confirmar se acham importante deixar ou nao\n",
    "\n",
    "\n",
    "During the computing process we need to creat various load systems to save time and make easier to re-start working where we have stoped before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from yellowbrick.cluster.elbow import KElbowVisualizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from IPython.display import YouTubeVideo\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1- Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the size and the weight of the dataset we had to load  the dataset in chunks of 10M Rows\n",
    "After the creation of each chunk we upload each chunk to a different csv fille to be easier to read the filles in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what could be donne with the dataset we load him with only 100 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(r'C:\\Users\\classroom\\Documents\\b5data\\data.csv', nrows=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previows table it's visible that 7/9 columns have information that will not be needed to the analisies and that make the dataset heavy.\n",
    "So we created a function that will clean those columms and make the dataset much lighter.\n",
    "To avoid memory problems we will run this cleaning function in the chunks and not in the all datset together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(ds):\n",
    "    ds['ProductFamily_ID'] = ds['ProductFamily_ID'].str.split('_').str[1]\n",
    "    ds['ProductCategory_ID'] = ds['ProductCategory_ID'].str.split('_').str[1]\n",
    "    ds['ProductBrand_ID'] = ds['ProductBrand_ID'].str.split('_').str[1]\n",
    "    ds['ProductName_ID'] = ds['ProductName_ID'].str.split('_').str[1]\n",
    "    ds['ProductPackSKU_ID'] = ds['ProductPackSKU_ID'].str.split('_').str[1]\n",
    "    ds['Point-of-Sale_ID'] = ds['Point-of-Sale_ID'].str.split('_').str[1]\n",
    "    ds['Measures'] = ds['Measures'].str.split(' ').str[1]\n",
    "    ds['Date'] = pd.to_datetime(ds['Date'])\n",
    "    dS = ds.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for no in range(1,20):\n",
    "    df = pd.read_csv('chunkfile' + str(no) + '.csv')\n",
    "    pre_process(df)\n",
    "    df.to_csv('chunk_after_preproc' + str(no) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correr mais tarde para testar\n",
    "a1 = pd.read_csv('chunk_after_preproc1.csv')\n",
    "\n",
    "for no in range(2,20):\n",
    "    a1 = a1.append(pd.read_csv('chunk_after_preproc' + str(no) + '.csv'))\n",
    "    \n",
    "\n",
    "\n",
    "a1.to_csv('merge1', index=False)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:/Users/frede/Documents/GitHub/BC5_Demand_Forecast/data/MindOverData_RetailChallenge.csv'\n",
    "pathPP = r'C:/Users/frede/Documents/GitHub/BC5_Demand_Forecast/data/values_units.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv(path, nrows=1999998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreProcessing - removing excessive letters from the variables\n",
    "\n",
    "data['ProductFamily_ID'] = data['ProductFamily_ID'].str.split('_').str[1]\n",
    "data['ProductCategory_ID'] = data['ProductCategory_ID'].str.split('_').str[1]\n",
    "data['ProductBrand_ID'] = data['ProductBrand_ID'].str.split('_').str[1]\n",
    "data['ProductName_ID'] = data['ProductName_ID'].str.split('_').str[1]\n",
    "data['ProductPackSKU_ID'] = data['ProductPackSKU_ID'].str.split('_').str[1]\n",
    "data['Point-of-Sale_ID'] = data['Point-of-Sale_ID'].str.split('_').str[1]\n",
    "\n",
    "data['Measures'] = data['Measures'].str.split(' ').str[1]\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "#data = data.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable with the day of the week of each purchase\n",
    "\n",
    "data['Day of the Week'] = data['Date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable with the correspondent quarter\n",
    "\n",
    "data['Quarter'] = data['Date'].dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataframe based on Units/ Values\n",
    "\n",
    "data_units = data[data['Measures'] == 'units']\n",
    "data_values = data[data['Measures'] == 'values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_values = data_values.assign(Units=data_units['Value'].values)\n",
    "data_values = data_values.drop(columns='Measures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_values.to_csv('values_units.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPP =  pd.read_csv(pathPP,dtype={'ProductFamily_ID': str, 'Point-of-Sale_ID': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.countplot(x=\"Day of the Week\", data=dataPP, color='darkblue')\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xlabel('Day of the week', fontsize=12)\n",
    "plt.title(\"Frequency of purchase by day of the week\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = dataPP['ProductName_ID'].value_counts()[:10]\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(cnt.values, cnt.index, alpha=0.8, color='gray')\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.ylabel('Product ID', fontsize=12)\n",
    "plt.title(\"Top 10 products\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(dataPP['Value'], color='green')\n",
    "plt.xlabel('Value', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quarterly analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### January, February, and March (Q1)\n",
    "###### April, May, and June (Q2)\n",
    "###### July, August, and September (Q3)\n",
    "###### October, November, and December (Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(dataPP.sort_values(by='Date'), x=\"Date\", y=\"Value\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(data_values, x=\"total_bill\", y=\"day\", orientation='h')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further exploration was done using Power BI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [\"ProductFamily_ID\", \"Point-of-Sale_ID\",\"ProductName_ID\",\"Units\",\"Date\",\"Quarter\"]\n",
    "dataPP =  pd.read_csv(pathPP, usecols=col_list, nrows=1000000,dtype={'ProductFamily_ID': str, 'Point-of-Sale_ID': str, 'ProductName_ID': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPP['Date']= pd.to_datetime(dataPP['Date'])\n",
    "dataPP['year'] = pd.DatetimeIndex(dataPP['Date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPP['Date'] = dataPP['Date'].astype(str)\n",
    "\n",
    "dataPP['TID'] = dataPP['ProductFamily_ID'] + dataPP['Point-of-Sale_ID'] + ' ' + dataPP['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mba(pos, qt, yr):\n",
    "    fam =  dataPP[(dataPP['Point-of-Sale_ID'] == pos) & (dataPP['Quarter'] == qt) & (dataPP['year'] == yr)]\n",
    "    fam = fam[['ProductName_ID','TID']]\n",
    "    fam_pivot = pd.pivot_table(fam, index='TID', columns='ProductName_ID', \n",
    "                    aggfunc=lambda x: 1 if len(x)>0 else 0).fillna(0)\n",
    "    frequent_itemsets = apriori(fam_pivot, min_support=0.05, use_colnames=True)\n",
    "    print(frequent_itemsets.sort_values(by='support', ascending=False).head(10))\n",
    "    rulesConfidence = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.50)\n",
    "    rulesConfidence.sort_values(by='confidence', ascending=False, inplace=True)\n",
    "    print(rulesConfidence.head())\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mba('1',1,2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar Pos / Quarter / Year\n",
    "fam =  dataPP[(dataPP['Point-of-Sale_ID'] == '2') & (dataPP['Quarter'] == 1) & (dataPP['year'] == 2016)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam = fam[['ProductName_ID','TID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_pivot = pd.pivot_table(fam, index='TID', columns='ProductName_ID', \n",
    "                    aggfunc=lambda x: 1 if len(x)>0 else 0).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fam.to_csv('fam_sample.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(fam_pivot, min_support=0.05, use_colnames=True)\n",
    "frequent_itemsets.sort_values(by='support', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the association rules - by confidence\n",
    "rulesConfidence = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.50)\n",
    "rulesConfidence.sort_values(by='confidence', ascending=False, inplace=True)\n",
    "rulesConfidence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the association rules - by lift\n",
    "rulesLift = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.5)\n",
    "rulesLift.sort_values(by='lift', ascending=False, inplace=True)\n",
    "rulesLift.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scatter = dict(type='scatter',\n",
    "    y=rulesConfidence['confidence'][:10],\n",
    "    x=rulesConfidence['support'][:10],\n",
    "    #text=rulesLift.consequents,\n",
    "    #mode='markers',\n",
    "    #marker=dict(\n",
    "    #size=rulesLift['support'],\n",
    "    hovertemplate='Lift: ' + rulesLift[\"lift\"].astype(str) + '<br>'+\n",
    "                    'Confidence: ' + rulesLift['confidence'].astype(str) + '<br>'+\n",
    "                    'Support: ' + rulesLift['support'].astype(str) + '<br>'+    \n",
    "                    'Antecedents: ' + rulesLift['antecedents'].astype(str) + '<br>'+\n",
    "                    'Consequents: ' + rulesLift['consequents'].astype(str) + '<br>'\n",
    "    '<extra></extra>',\n",
    "    mode='markers',\n",
    "    marker=dict(size=8,\n",
    "                color=rulesLift['lift'],\n",
    "                colorscale='dense',\n",
    "                showscale=True,\n",
    "                line_width=2),\n",
    "    )\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data_scatter)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10)) \n",
    "sns.scatterplot(data=rulesLift, x=\"confidence\", y=\"lift\", hue=\"support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_clusters(df,cluster_model):\n",
    "    metrics=['silhouette','distortion','calinski_harabasz']\n",
    "    model=cluster_model\n",
    "    for score in metrics:\n",
    "        plt.figure()\n",
    "        plt.ylabel(score)\n",
    "        plt.xlabel('Number of clusters')\n",
    "        Visualizer=KElbowVisualizer(model,metric=score).fit(df)\n",
    "        plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, compar_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    if compar_titles == None:\n",
    "        compar_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2, figsize=figsize, squeeze=False)\n",
    "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
    "        # Filtering df\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "        # Setting Data\n",
    "        pd.plotting.parallel_coordinates(centroids, label, color=sns.color_palette(), ax=ax[0])\n",
    "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1])\n",
    "\n",
    "        #Setting Layout\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy') \n",
    "        ax[0].legend(handles, cluster_labels) # Adaptable to number of clusters\n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=-20)\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_config(data,limit_nrcomponents):\n",
    "    bic_values =[]\n",
    "    aic_values =[]\n",
    "    types_covariance=['full','diag','spherical']\n",
    "    for i in types_covariance:\n",
    "        n_components = np.arange(1, limit_nrcomponents)\n",
    "        models = [GaussianMixture(n, covariance_type=i, n_init=10, random_state=1).fit(data)\n",
    "                  for n in n_components]\n",
    "        bic_values.append([m.bic(data) for m in models])\n",
    "        aic_values.append([m.aic(data) for m in models])\n",
    "    #barplot for BIC\n",
    "    bic=plt.figure(1)\n",
    "    X = np.arange(1,limit_nrcomponents)\n",
    "    plt.bar(X + 0.00, bic_values[0], label='full' ,color = 'b', width = 0.3)\n",
    "    plt.bar(X + 0.25, bic_values[1], label='diag',color = 'g', width = 0.3)\n",
    "    plt.bar(X + 0.50, bic_values[2], label='spherical',color = 'r', width = 0.3)\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('BIC')\n",
    "    plt.legend(loc='best')\n",
    "   #barplot for AIC\n",
    "    aic=plt.figure(2)\n",
    "    X = np.arange(1,limit_nrcomponents)\n",
    "    plt.bar(X + 0.00, aic_values[0], label='full' ,color = 'b', width = 0.3)\n",
    "    plt.bar(X + 0.25, aic_values[1], label='diag',color = 'g', width = 0.3)\n",
    "    plt.bar(X + 0.50, aic_values[2], label='spherical',color = 'r', width = 0.3)\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('AIC')\n",
    "    plt.legend(loc='best')\n",
    "    bic.show\n",
    "    aic.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2_hc(df, link_method, max_nclus, min_nclus=1, dist=\"euclidean\"):\n",
    "    \"\"\"This function computes the R2 for a set of cluster solutions given by the application of a hierarchical method.\n",
    "    The R2 is a measure of the homogenity of a cluster solution. It is based on SSt = SSw + SSb and R2 = SSb/SSt. \n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset to apply clustering\n",
    "    link_method (str): either \"ward\", \"complete\", \"average\", \"single\"\n",
    "    max_nclus (int): maximum number of clusters to compare the methods\n",
    "    min_nclus (int): minimum number of clusters to compare the methods. Defaults to 1.\n",
    "    dist (str): distance to use to compute the clustering solution. Must be a valid distance. Defaults to \"euclidean\".\n",
    "    \n",
    "    Returns:\n",
    "    ndarray: R2 values for the range of cluster solutions\n",
    "    \"\"\"\n",
    "    def get_ss(df):\n",
    "        ss = np.sum(df.var() * (df.count() - 1))\n",
    "        return ss  # return sum of sum of squares of each df variable\n",
    "    \n",
    "    sst = get_ss(df)  # get total sum of squares\n",
    "    \n",
    "    r2 = []  # where we will store the R2 metrics for each cluster solution\n",
    "    \n",
    "    for i in range(min_nclus, max_nclus+1):  # iterate over desired ncluster range\n",
    "        cluster = AgglomerativeClustering(n_clusters=i, affinity=dist, linkage=link_method)\n",
    "        hclabels = cluster.fit_predict(df) #get cluster labels\n",
    "        df_concat = pd.concat((df, pd.Series(hclabels, name='labels')), axis=1)  # concat df with labels\n",
    "        ssw_labels = df_concat.groupby(by='labels').apply(get_ss)  # compute ssw for each cluster labels\n",
    "        ssb = sst - np.sum(ssw_labels)  # remember: SST = SSW + SSB\n",
    "        r2.append(ssb / sst)  # save the R2 of the given cluster solution\n",
    "        \n",
    "    return np.array(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on 'type_covariance'-you have to insert with '', for example 'full'\n",
    "def gmm_components(data,type_covariance,min_components,max_components):\n",
    "    # Selecting number of components based on AIC and BIC\n",
    "    n_components = np.arange(min_components,max_components)\n",
    "    models = [GaussianMixture(n, covariance_type=type_covariance, n_init=10, random_state=1).fit(data)\n",
    "              for n in n_components]\n",
    "\n",
    "    bic_values = [m.bic(data) for m in models]\n",
    "    aic_values = [m.aic(data) for m in models]\n",
    "    plt.plot(n_components, bic_values, label='BIC')\n",
    "    plt.plot(n_components, aic_values, label='AIC')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('n_components')\n",
    "    plt.xticks(n_components)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet for clustering by value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\classroom\\OneDrive - NOVAIMS\\BCWDS\\BC5\\values_units.csv', usecols=['ProductName_ID','Point-of-Sale_ID','Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(df, values='Value', index=['Point-of-Sale_ID'],\n",
    "                    columns=['ProductName_ID'], aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the dataset to run the clustering methods\n",
    "names = table.columns\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_df = scaler.fit_transform(table)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=names)\n",
    "  \n",
    "# Normalizing the Data \n",
    "normalized_data = normalize(scaled_df) \n",
    "  \n",
    "# Converting the numpy array into a pandas DataFrame \n",
    "normalized_data = pd.DataFrame(normalized_data,columns=names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet for clustering by product preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(r'C:\\Users\\classroom\\OneDrive - NOVAIMS\\BCWDS\\BC5\\values_units.csv', usecols=['ProductName_ID','Point-of-Sale_ID','Units'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = pd.pivot_table(df2, values='Units', index=['Point-of-Sale_ID'],\n",
    "                    columns=['ProductName_ID'], aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = table2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the dataset to run the clustering methods\n",
    "names = table2.columns\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_df = scaler.fit_transform(table2)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=names)\n",
    "  \n",
    "# Normalizing the Data \n",
    "normalized_data2 = normalize(scaled_df) \n",
    "  \n",
    "# Converting the numpy array into a pandas DataFrame \n",
    "normalized_data2 = pd.DataFrame(normalized_data2,columns=names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering for value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans\n",
    "\n",
    "\n",
    "K-Elbow plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters(normalized_data ,KMeans( init='k-means++', n_init=15, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[SilhouetteVisualizer(KMeans(n_clusters=i,init='k-means++', n_init=15, random_state=1), colors='yellowbrick').fit(normalized_data).show() for i in range(2,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#According to the KElbowVisualizer and the SilhouetteVisualizer we were able to determine that the optimal number of clusters is 4.\n",
    "#confirmar nº de k\n",
    "kmclust = KMeans(n_clusters=3, init='k-means++', n_init=15, random_state=1)\n",
    "km_labels = kmclust.fit_predict(normalized_data)\n",
    "df_concat = pd.concat((normalized_data, pd.Series(km_labels, name='labels')), axis=1)\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a copy of the dataset and appending a column with the cluster assigned to the observation.\n",
    "km_df = normalized_data.copy()\n",
    "km_df[\"km_labels\"] = km_labels\n",
    "km_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMM\n",
    "\n",
    "Choosing the type of covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_config(normalized_data,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, the number of components the model with the lowest AIC and BIC is 'diag' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing GMM clustering\n",
    "gmm = GaussianMixture(n_components=3, covariance_type='diag', n_init=10, init_params='kmeans', random_state=1)\n",
    "gmm_labels = gmm.fit_predict(normalized_data)\n",
    "# Concatenating the labels to df\n",
    "df_concat = pd.concat([normalized_data, pd.Series(gmm_labels, index=normalized_data.index, name=\"gmm_labels\")], axis=1)\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_ = normalized_data.copy()\n",
    "gmm_[\"gmm_labels\"] = gmm_labels\n",
    "gmm_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarquical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing HC\n",
    "hclust = AgglomerativeClustering(linkage='ward', affinity='euclidean', n_clusters=5)\n",
    "hc_labels = hclust.fit_predict(normalized_data)\n",
    "hc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing SST\n",
    "X = normalized_data.values\n",
    "sst = np.sum(np.square(X - X.mean(axis=0)), axis=0)\n",
    "\n",
    "# Computing SSW\n",
    "ssw_iter = []\n",
    "for i in np.unique(hc_labels):\n",
    "    X_k = X[hc_labels == i]\n",
    "    ssw_iter.append(np.sum(np.square(X_k - X_k.mean(axis=0)), axis=0))\n",
    "ssw = np.sum(ssw_iter, axis=0)\n",
    "\n",
    "# Computing SSB\n",
    "ssb_iter = []\n",
    "for i in np.unique(hc_labels):\n",
    "    X_k = X[hc_labels == i]\n",
    "    ssb_iter.append(X_k.shape[0] * np.square(X_k.mean(axis=0) - X.mean(axis=0)))\n",
    "ssb = np.sum(ssb_iter, axis=0)\n",
    "\n",
    "# Verifying the formula\n",
    "np.round(sst) == np.round((ssw + ssb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input\n",
    "hc_methods = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "# Call function defined above to obtain the R2 statistic for each hc_method\n",
    "max_nclus = 10\n",
    "r2_hc_methods = np.vstack([get_r2_hc(df=normalized_data, link_method=link, max_nclus=max_nclus) for link in hc_methods]).T\n",
    "r2_hc_methods = pd.DataFrame(r2_hc_methods, index=range(1, max_nclus + 1), columns=hc_methods)\n",
    "\n",
    "sns.set()\n",
    "# Plot data\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "sns.lineplot(data=r2_hc_methods, linewidth=2.5, markers=[\"o\"]*4)\n",
    "\n",
    "# Finalize the plot\n",
    "fig.suptitle(\"R2 plot for various hierarchical methods\", fontsize=21)\n",
    "plt.gca().invert_xaxis()  # invert x axis\n",
    "plt.legend(title=\"HC methods\", title_fontsize=11)\n",
    "plt.xticks(range(1, max_nclus + 1))\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R2 metric\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting distance_threshold=0 and n_clusters=None ensures we compute the full tree\n",
    "linkage = 'complete'\n",
    "distance = 'euclidean'\n",
    "hclust = AgglomerativeClustering(linkage=linkage, affinity=distance, distance_threshold=0, n_clusters=None)\n",
    "hclust.fit_predict(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold = 1.57\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'{distance.title()} Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 cluster solution\n",
    "linkage = 'average'\n",
    "distance = 'euclidean'\n",
    "hc3lust = AgglomerativeClustering(linkage=linkage, affinity=distance, n_clusters=3)\n",
    "hc3_labels = hc3lust.fit_predict(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the 3 clusters\n",
    "HC= pd.concat((normalized_data, pd.Series(hc3_labels, name='labels')), axis=1)\n",
    "HC.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrica para comparar clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss(df):\n",
    "    ss = np.sum(df.var() * (df.count() - 1))\n",
    "    return ss  # return sum of sum of squares of each df variable\n",
    "def r2(df, labels):\n",
    "    sst = get_ss(df)\n",
    "    ssw = np.sum(df.groupby(labels).apply(get_ss))\n",
    "    return 1 - ssw/sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_list = [r2(km_df, 'km_labels'), r2(gmm_, 'gmm_labels'), r2(HC, 'labels')]\n",
    "r2_tograph = np.asarray(r2_list)\n",
    "# Create a List of Labels for x-axis\n",
    "names = [\"K-Means\", \"Gmm\",\"HC\"]\n",
    "# Make the Chart\n",
    "plt.bar(names, r2_tograph, color=['black', 'red', 'green'])\n",
    "plt.title('Comparison of the different clustering methods')\n",
    "plt.xlabel('Clustering method')\n",
    "plt.ylabel('R2')\n",
    "# Show the Chart\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('The r2 of the k-means method is', r2(km_df, 'km_labels').round(2))\n",
    "print('The r2 of the GMM method is', r2(gmm_, 'gmm_labels').round(2))\n",
    "print('The r2 of the HC method is', r2(HC, 'labels').round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due the fact that the all cluster methods have the same number of clusters we can choose the best method by comparing the r^2 of each method\n",
    "After analyse the previous barplot we came to the conclusion that the HC is the best method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clusters to analyse\n",
    "table = table.reset_index()\n",
    "HC_final = pd.concat((table , pd.Series(hc3_labels, name='labels')), axis=1)\n",
    "HC_final = HC_final.set_index('Point-of-Sale_ID')\n",
    "HC_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the clusters to analyse later\n",
    "cluster10 = HC_final.loc[HC_final['labels'] == 0]\n",
    "cluster11 = HC_final.loc[HC_final['labels'] == 1]\n",
    "cluster12 = HC_final.loc[HC_final['labels'] == 2]\n",
    "\n",
    "\n",
    "#check if we didn't lost any POS\n",
    "len(HC_final) == len(cluster11) + len(cluster12)  +len(cluster10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the clusters in a csv to use later \n",
    "\n",
    "cluster10.to_csv('cluster10.csv', index=False)\n",
    "cluster11.to_csv('cluster11.csv', index=False)\n",
    "cluster12.to_csv('cluster12.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering for product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans\n",
    "\n",
    "K-Elbow plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters(normalized_data2 ,KMeans( init='k-means++', n_init=15, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[SilhouetteVisualizer(KMeans(n_clusters=i,init='k-means++', n_init=15, random_state=1), colors='yellowbrick').fit(normalized_data2).show() for i in range(2,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#According to the KElbowVisualizer and the SilhouetteVisualizer we were able to determine that the optimal number of clusters is 4.\n",
    "kmclust = KMeans(n_clusters=4, init='k-means++', n_init=15, random_state=1)\n",
    "km_labels = kmclust.fit_predict(normalized_data2)\n",
    "df_concat = pd.concat((normalized_data2, pd.Series(km_labels, name='labels')), axis=1)\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a copy of the dataset and appending a column with the cluster assigned to the observation.\n",
    "km_df2 = normalized_data.copy()\n",
    "km_df2[\"km_labels\"] = km_labels\n",
    "km_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM\n",
    "\n",
    "Choosing the type of covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_config(normalized_data2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, the number of components the model with the lowest AIC and BIC is 'diag' \n",
    "\n",
    "\n",
    "Find the number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_components(normalized_data2,'diag',1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the graph, the teams considers the optimal number of components is 5.\n",
    "\n",
    "Applying the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing GMM clustering\n",
    "gmm = GaussianMixture(n_components=5, covariance_type='diag', n_init=10, init_params='kmeans', random_state=1)\n",
    "gmm_labels = gmm.fit_predict(normalized_data)\n",
    "# Concatenating the labels to df\n",
    "df_concat = pd.concat([normalized_data, pd.Series(gmm_labels, index=normalized_data.index, name=\"gmm_labels2\")], axis=1)\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_2= normalized_data.copy()\n",
    "gmm_2[\"gmm_labels2\"] = df_concat['gmm_labels2']\n",
    "gmm_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing HC\n",
    "hclust = AgglomerativeClustering(linkage='ward', affinity='euclidean', n_clusters=5)\n",
    "hc_labels = hclust.fit_predict(normalized_data2)\n",
    "hc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing SST\n",
    "X = normalized_data2.values\n",
    "sst = np.sum(np.square(X - X.mean(axis=0)), axis=0)\n",
    "\n",
    "# Computing SSW\n",
    "ssw_iter = []\n",
    "for i in np.unique(hc_labels):\n",
    "    X_k = X[hc_labels == i]\n",
    "    ssw_iter.append(np.sum(np.square(X_k - X_k.mean(axis=0)), axis=0))\n",
    "ssw = np.sum(ssw_iter, axis=0)\n",
    "\n",
    "# Computing SSB\n",
    "ssb_iter = []\n",
    "for i in np.unique(hc_labels):\n",
    "    X_k = X[hc_labels == i]\n",
    "    ssb_iter.append(X_k.shape[0] * np.square(X_k.mean(axis=0) - X.mean(axis=0)))\n",
    "ssb = np.sum(ssb_iter, axis=0)\n",
    "\n",
    "# Verifying the formula\n",
    "np.round(sst) == np.round((ssw + ssb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input\n",
    "hc_methods = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "# Call function defined above to obtain the R2 statistic for each hc_method\n",
    "max_nclus = 10\n",
    "r2_hc_methods = np.vstack([get_r2_hc(df=normalized_data2, link_method=link, max_nclus=max_nclus) for link in hc_methods]).T\n",
    "r2_hc_methods = pd.DataFrame(r2_hc_methods, index=range(1, max_nclus + 1), columns=hc_methods)\n",
    "\n",
    "sns.set()\n",
    "# Plot data\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "sns.lineplot(data=r2_hc_methods, linewidth=2.5, markers=[\"o\"]*4)\n",
    "\n",
    "# Finalize the plot\n",
    "fig.suptitle(\"R2 plot for various hierarchical methods\", fontsize=21)\n",
    "plt.gca().invert_xaxis()  # invert x axis\n",
    "plt.legend(title=\"HC methods\", title_fontsize=11)\n",
    "plt.xticks(range(1, max_nclus + 1))\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R2 metric\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting distance_threshold=0 and n_clusters=None ensures we compute the full tree\n",
    "linkage = 'ward'\n",
    "distance = 'euclidean'\n",
    "hclust = AgglomerativeClustering(linkage=linkage, affinity=distance, distance_threshold=0, n_clusters=None)\n",
    "hclust.fit_predict(normalized_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold = 2.75\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'{distance.title()} Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 cluster solution\n",
    "linkage = 'average'\n",
    "distance = 'euclidean'\n",
    "hc3lust = AgglomerativeClustering(linkage=linkage, affinity=distance, n_clusters=4)\n",
    "hc3_labels = hc3lust.fit_predict(normalized_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the 3 clusters\n",
    "HC2= pd.concat((normalized_data, pd.Series(hc3_labels, name='labels')), axis=1)\n",
    "HC2.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrica para comparar clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_list = [r2(km_df2, 'km_labels'), r2(gmm_2, 'gmm_labels'), r2(HC2, 'labels')]\n",
    "r_tograph = np.asarray(r_list)\n",
    "# Create a List of Labels for x-axis\n",
    "names = [\"K-Means\", \"Gmm\",\"HC\"]\n",
    "# Make the Chart\n",
    "plt.bar(names, r_tograph, color=['black', 'red', 'green'])\n",
    "plt.title('Comparison of the different clustering methods')\n",
    "plt.xlabel('Clustering method')\n",
    "plt.ylabel('R2')\n",
    "# Show the Chart\n",
    "plt.show()\n",
    "\n",
    "print('The r2 of the k-means method is', r2(km_df2, 'km_labels').round(2))\n",
    "print('The r2 of the GMM method is', r2(gmm_2, 'gmm_labels').round(2))\n",
    "print('The r2 of the HC method is', r2(HC2, 'labels').round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due the fact that the all cluster methods have the same number of variables we can choose the best method by comparing the r^2 of each method After analyse the previous barplot we came to the conclusion that the K-Means is the best method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clusters to analyse\n",
    "#table2 = table2.reset_index()\n",
    "final_cluster20 = pd.concat((table2 , pd.Series(gmm_labels, name='gmm_labels')), axis=1)\n",
    "final_cluster20 = final_cluster20.set_index('Point-of-Sale_ID')\n",
    "final_cluster20 = final_cluster20.drop(['index'], axis=1)\n",
    "final_cluster20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the clusters to analyse later\n",
    "cluster20 = final_cluster20.loc[final_cluster20['gmm_labels'] == 0]\n",
    "cluster21 = final_cluster20.loc[final_cluster20['gmm_labels'] == 1]\n",
    "cluster22 = final_cluster20.loc[final_cluster20['gmm_labels'] == 2]\n",
    "cluster23 = final_cluster20.loc[final_cluster20['gmm_labels'] == 3]\n",
    "cluster24 = final_cluster20.loc[final_cluster20['gmm_labels'] == 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the clusters to a csv to load lately\n",
    "cluster20.to_csv('cluster20.csv', index=False)\n",
    "cluster21.to_csv('cluster21.csv', index=False)\n",
    "cluster22.to_csv('cluster22.csv', index=False)\n",
    "cluster23.to_csv('cluster23.csv', index=False)\n",
    "cluster24.to_csv('cluster24.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters Analises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value\n",
    "LowValuePOS = pd.read_csv(r'cluster10.csv')\n",
    "MediumValuePOS = pd.read_csv(r'cluster11.csv')\n",
    "HighValuesPOS = pd.read_csv(r'cluster12.csv')\n",
    "\n",
    "#Units\n",
    "cluster20 = pd.read_csv(r'cluster20.csv')\n",
    "cluster21 = pd.read_csv(r'cluster21.csv')\n",
    "cluster22 = pd.read_csv(r'cluster22.csv')\n",
    "cluster23 = pd.read_csv(r'cluster23.csv')\n",
    "cluster24 = pd.read_csv(r'cluster24.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per value\n",
    "Distribution of the number of pos in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusters distribution\n",
    "list1 = [len(LowValuePOS),len(MediumValuePOS),len(HighValuesPOS)]\n",
    "list1 = np.asarray(list1)\n",
    "# Create a List of Labels for x-axis\n",
    "names = [\"LowValuePOS\", \"MediumValuePOS\",\"HighValuesPOS\"]\n",
    "# Make the Chart\n",
    "plt.bar(names, list1)\n",
    "plt.title('clusters distribution')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Total of POS in each cluster')\n",
    "# Show the Chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The medium value of sells of the POS in the LowValuePOS are', LowValuePOS.mean().mean(), 'and the total value is', LowValuePOS.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The medium value of sells of the POS in the LowValuePOS are', MediumValuePOS.mean().mean(), 'and the total value is', MediumValuePOS.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The medium value of sells of the POS in the LowValuePOS are', HighValuesPOS.mean().mean(), 'and the total value is', HighValuesPOS.sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per product preference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of the number of pos in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#clusters distribution\n",
    "list1 = [len(cluster20),len(cluster21),len(cluster22),len(cluster23),len(cluster24)]\n",
    "list1 = np.asarray(list1)\n",
    "# Create a List of Labels for x-axis\n",
    "names = [\"Cluster 0\", \"Cluster 1\",\"Cluster 2\", 'Cluster 3', 'Cluster 4']\n",
    "# Make the Chart\n",
    "plt.bar(names, list1)\n",
    "plt.title('clusters distribution')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Total of POS in each cluster')\n",
    "# Show the Chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster20.sum().sort_values(ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster21.sum().sort_values(ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster22.sum().sort_values(ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster23.sum().sort_values(ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster24.sum().sort_values(ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
